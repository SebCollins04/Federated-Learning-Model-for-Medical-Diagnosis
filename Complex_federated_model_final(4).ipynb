{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Author: Sebastian Collins (Newcastle University)\n",
        "# Date: May 2025\n",
        "# Description: Federated learning model for medical diagnosis with/without differential privacy Complex Model"
      ],
      "metadata": {
        "id": "BKOcB1YvFpbk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "siBepgnlbT9t",
        "outputId": "2845beed-4c40-40b4-f540-a3f57ad460e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "drwxr-xr-x 21 root root     4096 Apr 24 23:02 scipy\n",
            "drwxr-xr-x  2 root root     4096 Apr 24 23:02 scipy-1.9.3.dist-info\n",
            "drwxr-xr-x  2 root root     4096 Apr 24 23:02 scipy.libs\n",
            "Found existing installation: jax 0.4.14\n",
            "Uninstalling jax-0.4.14:\n",
            "  Successfully uninstalled jax-0.4.14\n",
            "Found existing installation: jaxlib 0.4.14\n",
            "Uninstalling jaxlib-0.4.14:\n",
            "  Successfully uninstalled jaxlib-0.4.14\n",
            "\u001b[33mWARNING: Skipping flax as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping optax as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping orbax-checkpoint as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mFound existing installation: ml-dtypes 0.2.0\n",
            "Uninstalling ml-dtypes-0.2.0:\n",
            "  Successfully uninstalled ml-dtypes-0.2.0\n",
            "Found existing installation: scipy 1.9.3\n",
            "Uninstalling scipy-1.9.3:\n",
            "  Successfully uninstalled scipy-1.9.3\n",
            "Found existing installation: tensorflow 2.14.0\n",
            "Uninstalling tensorflow-2.14.0:\n",
            "  Successfully uninstalled tensorflow-2.14.0\n",
            "Found existing installation: tensorflow_federated 0.84.0\n",
            "Uninstalling tensorflow_federated-0.84.0:\n",
            "  Successfully uninstalled tensorflow_federated-0.84.0\n",
            "\u001b[33mWARNING: Skipping chex as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mFound existing installation: numpy 1.25.2\n",
            "Uninstalling numpy-1.25.2:\n",
            "  Successfully uninstalled numpy-1.25.2\n",
            "\u001b[33mWARNING: Skipping pandas as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping matplotlib as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mFound existing installation: scikit-learn 1.6.1\n",
            "Uninstalling scikit-learn-1.6.1:\n",
            "  Successfully uninstalled scikit-learn-1.6.1\n",
            "\u001b[33mWARNING: Skipping imbalanced-learn as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting tensorflow==2.14.0\n",
            "  Using cached tensorflow-2.14.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
            "Collecting tensorflow-federated==0.84.0\n",
            "  Using cached tensorflow_federated-0.84.0-py3-none-manylinux_2_31_x86_64.whl.metadata (19 kB)\n",
            "Requirement already satisfied: tensorflow-addons==0.22.0 in /usr/local/lib/python3.11/dist-packages (0.22.0)\n",
            "Requirement already satisfied: tensorflow-privacy==0.9.0 in /usr/local/lib/python3.11/dist-packages (0.9.0)\n",
            "Collecting scipy==1.9.3\n",
            "  Using cached scipy-1.9.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (58 kB)\n",
            "Collecting ml-dtypes==0.2.0\n",
            "  Using cached ml_dtypes-0.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.14.0) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.14.0) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.14.0) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.14.0) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.14.0) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.14.0) (3.13.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.14.0) (18.1.1)\n",
            "Collecting numpy>=1.23.5 (from tensorflow==2.14.0)\n",
            "  Using cached numpy-2.2.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.14.0) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.14.0) (22.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.14.0) (4.25.7)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.14.0) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.14.0) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.14.0) (3.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.14.0) (4.5.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.14.0) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.14.0) (0.37.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.14.0) (1.71.0)\n",
            "Requirement already satisfied: tensorboard<2.15,>=2.14 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.14.0) (2.14.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.15,>=2.14.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.14.0) (2.14.0)\n",
            "Requirement already satisfied: keras<2.15,>=2.14.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.14.0) (2.14.0)\n",
            "Requirement already satisfied: attrs~=23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow-federated==0.84.0) (23.1.0)\n",
            "Requirement already satisfied: cachetools~=5.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow-federated==0.84.0) (5.5.2)\n",
            "Requirement already satisfied: dm-tree==0.1.8 in /usr/local/lib/python3.11/dist-packages (from tensorflow-federated==0.84.0) (0.1.8)\n",
            "Requirement already satisfied: dp-accounting==0.4.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow-federated==0.84.0) (0.4.3)\n",
            "Requirement already satisfied: google-vizier==0.1.11 in /usr/local/lib/python3.11/dist-packages (from tensorflow-federated==0.84.0) (0.1.11)\n",
            "Collecting jaxlib==0.4.14 (from tensorflow-federated==0.84.0)\n",
            "  Using cached jaxlib-0.4.14-cp311-cp311-manylinux2014_x86_64.whl.metadata (2.0 kB)\n",
            "Collecting jax==0.4.14 (from tensorflow-federated==0.84.0)\n",
            "  Using cached jax-0.4.14-py3-none-any.whl\n",
            "Collecting numpy>=1.23.5 (from tensorflow==2.14.0)\n",
            "  Using cached numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "Requirement already satisfied: portpicker~=1.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow-federated==0.84.0) (1.6.0)\n",
            "Requirement already satisfied: tensorflow-model-optimization==0.7.5 in /usr/local/lib/python3.11/dist-packages (from tensorflow-federated==0.84.0) (0.7.5)\n",
            "Requirement already satisfied: tqdm~=4.64 in /usr/local/lib/python3.11/dist-packages (from tensorflow-federated==0.84.0) (4.67.1)\n",
            "Requirement already satisfied: googleapis-common-protos==1.61.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow-federated==0.84.0) (1.61.0)\n",
            "Requirement already satisfied: typeguard<3.0.0,>=2.7 in /usr/local/lib/python3.11/dist-packages (from tensorflow-addons==0.22.0) (2.13.3)\n",
            "Collecting scikit-learn==1.*,>=1.0 (from tensorflow-privacy==0.9.0)\n",
            "  Using cached scikit_learn-1.6.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
            "Requirement already satisfied: tensorflow-probability~=0.22.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow-privacy==0.9.0) (0.22.1)\n",
            "Collecting numpy>=1.23.5 (from tensorflow==2.14.0)\n",
            "  Using cached numpy-1.25.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: mpmath~=1.2 in /usr/local/lib/python3.11/dist-packages (from dp-accounting==0.4.3->tensorflow-federated==0.84.0) (1.3.0)\n",
            "Requirement already satisfied: grpcio-tools>=1.35.0 in /usr/local/lib/python3.11/dist-packages (from google-vizier==0.1.11->tensorflow-federated==0.84.0) (1.62.3)\n",
            "Requirement already satisfied: sqlalchemy<=1.4.20,>=1.4 in /usr/local/lib/python3.11/dist-packages (from google-vizier==0.1.11->tensorflow-federated==0.84.0) (1.4.20)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.*,>=1.0->tensorflow-privacy==0.9.0) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.*,>=1.0->tensorflow-privacy==0.9.0) (3.6.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow==2.14.0) (0.45.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from portpicker~=1.6->tensorflow-federated==0.84.0) (5.9.5)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.15,>=2.14->tensorflow==2.14.0) (2.38.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.15,>=2.14->tensorflow==2.14.0) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.15,>=2.14->tensorflow==2.14.0) (3.8)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.15,>=2.14->tensorflow==2.14.0) (2.32.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.15,>=2.14->tensorflow==2.14.0) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.15,>=2.14->tensorflow==2.14.0) (3.1.3)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from tensorflow-probability~=0.22.0->tensorflow-privacy==0.9.0) (4.4.2)\n",
            "Requirement already satisfied: cloudpickle>=1.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow-probability~=0.22.0->tensorflow-privacy==0.9.0) (3.1.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow==2.14.0) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow==2.14.0) (4.9.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow==2.14.0) (2.0.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow==2.14.0) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow==2.14.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow==2.14.0) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow==2.14.0) (2025.1.31)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy<=1.4.20,>=1.4->google-vizier==0.1.11->tensorflow-federated==0.84.0) (3.2.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.15,>=2.14->tensorflow==2.14.0) (3.0.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow==2.14.0) (0.6.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow==2.14.0) (3.2.2)\n",
            "Using cached tensorflow-2.14.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (489.9 MB)\n",
            "Using cached tensorflow_federated-0.84.0-py3-none-manylinux_2_31_x86_64.whl (71.8 MB)\n",
            "Using cached scipy-1.9.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (33.4 MB)\n",
            "Using cached ml_dtypes-0.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "Using cached jaxlib-0.4.14-cp311-cp311-manylinux2014_x86_64.whl (73.7 MB)\n",
            "Using cached scikit_learn-1.6.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.5 MB)\n",
            "Using cached numpy-1.25.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
            "Installing collected packages: numpy, scipy, ml-dtypes, scikit-learn, jaxlib, jax, tensorflow, tensorflow-federated\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "geemap 0.35.3 requires matplotlib, which is not installed.\n",
            "geemap 0.35.3 requires pandas, which is not installed.\n",
            "geopandas 1.0.1 requires pandas>=1.4.0, which is not installed.\n",
            "bigframes 2.1.0 requires matplotlib>=3.7.1, which is not installed.\n",
            "bigframes 2.1.0 requires pandas>=1.5.3, which is not installed.\n",
            "statsmodels 0.14.4 requires pandas!=2.1.0,>=1.4, which is not installed.\n",
            "xarray 2025.1.2 requires pandas>=2.1, which is not installed.\n",
            "pycocotools 2.0.8 requires matplotlib>=2.1.0, which is not installed.\n",
            "sklearn-pandas 2.2.0 requires pandas>=1.1.4, which is not installed.\n",
            "cmdstanpy 1.2.5 requires pandas, which is not installed.\n",
            "arviz 0.21.0 requires matplotlib>=3.5, which is not installed.\n",
            "arviz 0.21.0 requires pandas>=1.5.0, which is not installed.\n",
            "db-dtypes 1.4.2 requires pandas>=0.24.2, which is not installed.\n",
            "pymc 5.21.2 requires pandas>=0.24.0, which is not installed.\n",
            "bqplot 0.12.44 requires pandas<3.0.0,>=1.0.0, which is not installed.\n",
            "dopamine-rl 4.1.2 requires flax>=0.2.0, which is not installed.\n",
            "dopamine-rl 4.1.2 requires pandas>=0.24.2, which is not installed.\n",
            "plotnine 0.14.5 requires matplotlib>=3.8.0, which is not installed.\n",
            "plotnine 0.14.5 requires pandas>=2.2.0, which is not installed.\n",
            "holoviews 1.20.2 requires pandas>=1.3, which is not installed.\n",
            "prophet 1.1.6 requires matplotlib>=2.0.0, which is not installed.\n",
            "prophet 1.1.6 requires pandas>=1.0.4, which is not installed.\n",
            "tensorflow-decision-forests 1.11.0 requires pandas, which is not installed.\n",
            "matplotlib-venn 1.1.2 requires matplotlib, which is not installed.\n",
            "yellowbrick 1.5 requires matplotlib!=3.0.0,>=2.0.2, which is not installed.\n",
            "bigquery-magics 0.9.0 requires pandas>=1.1.0, which is not installed.\n",
            "datascience 0.17.6 requires matplotlib>=3.0.0, which is not installed.\n",
            "datascience 0.17.6 requires pandas, which is not installed.\n",
            "cudf-cu12 25.2.1 requires pandas<2.2.4dev0,>=2.0, which is not installed.\n",
            "pandas-gbq 0.28.0 requires pandas>=1.1.4, which is not installed.\n",
            "shap 0.47.2 requires pandas, which is not installed.\n",
            "yfinance 0.2.55 requires pandas>=1.3.0, which is not installed.\n",
            "dask-cuda 25.2.0 requires pandas>=1.3, which is not installed.\n",
            "dask-cudf-cu12 25.2.2 requires pandas<2.2.4dev0,>=2.0, which is not installed.\n",
            "wordcloud 1.9.4 requires matplotlib, which is not installed.\n",
            "mizani 0.13.3 requires pandas>=2.2.0, which is not installed.\n",
            "missingno 0.5.2 requires matplotlib, which is not installed.\n",
            "mlxtend 0.23.4 requires matplotlib>=3.0.0, which is not installed.\n",
            "mlxtend 0.23.4 requires pandas>=0.24.2, which is not installed.\n",
            "cufflinks 0.17.3 requires pandas>=0.19.2, which is not installed.\n",
            "fastai 2.7.19 requires matplotlib, which is not installed.\n",
            "fastai 2.7.19 requires pandas, which is not installed.\n",
            "bokeh 3.6.3 requires pandas>=1.2, which is not installed.\n",
            "music21 9.3.0 requires matplotlib, which is not installed.\n",
            "seaborn 0.13.2 requires matplotlib!=3.6.1,>=3.4, which is not installed.\n",
            "seaborn 0.13.2 requires pandas>=1.2, which is not installed.\n",
            "tf-keras 2.18.0 requires tensorflow<2.19,>=2.18, but you have tensorflow 2.14.0 which is incompatible.\n",
            "tensorflow-text 2.18.1 requires tensorflow<2.19,>=2.18.0, but you have tensorflow 2.14.0 which is incompatible.\n",
            "xarray 2025.1.2 requires packaging>=23.2, but you have packaging 22.0 which is incompatible.\n",
            "cvxpy 1.6.5 requires scipy>=1.11.0, but you have scipy 1.9.3 which is incompatible.\n",
            "tensorstore 0.1.73 requires ml_dtypes>=0.3.1, but you have ml-dtypes 0.2.0 which is incompatible.\n",
            "albumentations 2.0.5 requires scipy>=1.10.0, but you have scipy 1.9.3 which is incompatible.\n",
            "tensorflow-decision-forests 1.11.0 requires tensorflow==2.18.0, but you have tensorflow 2.14.0 which is incompatible.\n",
            "nibabel 5.3.2 requires typing-extensions>=4.6; python_version < \"3.13\", but you have typing-extensions 4.5.0 which is incompatible.\n",
            "blosc2 3.3.1 requires numpy>=1.26, but you have numpy 1.25.2 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.25.2 which is incompatible.\n",
            "ydf 0.11.0 requires protobuf<6.0.0,>=5.29.1, but you have protobuf 4.25.7 which is incompatible.\n",
            "scikit-image 0.25.2 requires scipy>=1.11.4, but you have scipy 1.9.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed jax-0.4.14 jaxlib-0.4.14 ml-dtypes-0.2.0 numpy-1.25.2 scikit-learn-1.6.1 scipy-1.9.3 tensorflow-2.14.0 tensorflow-federated-0.84.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "339ec6e3ae764f7485bdb51aa2c7a8dd"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting jax==0.4.27\n",
            "  Using cached jax-0.4.27-py3-none-any.whl.metadata (23 kB)\n",
            "Collecting jaxlib==0.4.27\n",
            "  Using cached jaxlib-0.4.27-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting flax==0.7.2\n",
            "  Using cached flax-0.7.2-py3-none-any.whl.metadata (10.0 kB)\n",
            "Collecting optax==0.1.7\n",
            "  Using cached optax-0.1.7-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting orbax-checkpoint==0.11.10\n",
            "  Using cached orbax_checkpoint-0.11.10-py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from jax==0.4.27) (0.2.0)\n",
            "Requirement already satisfied: numpy>=1.22 in /usr/local/lib/python3.11/dist-packages (from jax==0.4.27) (1.25.2)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.11/dist-packages (from jax==0.4.27) (3.4.0)\n",
            "Requirement already satisfied: scipy>=1.9 in /usr/local/lib/python3.11/dist-packages (from jax==0.4.27) (1.9.3)\n",
            "Requirement already satisfied: msgpack in /usr/local/lib/python3.11/dist-packages (from flax==0.7.2) (1.1.0)\n",
            "Requirement already satisfied: tensorstore in /usr/local/lib/python3.11/dist-packages (from flax==0.7.2) (0.1.73)\n",
            "Requirement already satisfied: rich>=11.1 in /usr/local/lib/python3.11/dist-packages (from flax==0.7.2) (13.9.4)\n",
            "Requirement already satisfied: typing-extensions>=4.1.1 in /usr/local/lib/python3.11/dist-packages (from flax==0.7.2) (4.5.0)\n",
            "Requirement already satisfied: PyYAML>=5.4.1 in /usr/local/lib/python3.11/dist-packages (from flax==0.7.2) (6.0.2)\n",
            "Requirement already satisfied: absl-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from optax==0.1.7) (1.4.0)\n",
            "Collecting chex>=0.1.5 (from optax==0.1.7)\n",
            "  Using cached chex-0.1.89-py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: etils[epath,epy] in /usr/local/lib/python3.11/dist-packages (from orbax-checkpoint==0.11.10) (1.12.2)\n",
            "INFO: pip is looking at multiple versions of orbax-checkpoint to determine which version is compatible with other requirements. This could take a while.\n",
            "\u001b[31mERROR: Cannot install flax==0.7.2, jax==0.4.27, optax==0.1.7 and orbax-checkpoint==0.11.10 because these package versions have conflicting dependencies.\u001b[0m\u001b[31m\n",
            "\u001b[0m\n",
            "The conflict is caused by:\n",
            "    The user requested jax==0.4.27\n",
            "    flax 0.7.2 depends on jax>=0.4.2\n",
            "    optax 0.1.7 depends on jax>=0.1.55\n",
            "    orbax-checkpoint 0.11.10 depends on jax>=0.5.0\n",
            "\n",
            "To fix this you could try to:\n",
            "1. loosen the range of package versions you've specified\n",
            "2. remove package versions to allow pip to attempt to solve the dependency conflict\n",
            "\n",
            "\u001b[31mERROR: ResolutionImpossible: for help visit https://pip.pypa.io/en/latest/topics/dependency-resolution/#dealing-with-dependency-conflicts\u001b[0m\u001b[31m\n",
            "\u001b[0mRequirement already satisfied: numpy==1.25.2 in /usr/local/lib/python3.11/dist-packages (1.25.2)\n",
            "Collecting pandas==2.2.3\n",
            "  Using cached pandas-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n",
            "Collecting matplotlib==3.10.1\n",
            "  Using cached matplotlib-3.10.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Requirement already satisfied: scikit-learn==1.6.1 in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Collecting imbalanced-learn==0.13.0\n",
            "  Using cached imbalanced_learn-0.13.0-py3-none-any.whl.metadata (8.8 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas==2.2.3) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas==2.2.3) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas==2.2.3) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.10.1) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.10.1) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.10.1) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.10.1) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.10.1) (22.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.10.1) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.10.1) (3.2.3)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.6.1) (1.9.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.6.1) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.6.1) (3.6.0)\n",
            "Collecting scipy>=1.6.0 (from scikit-learn==1.6.1)\n",
            "  Downloading scipy-1.15.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m427.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sklearn-compat<1,>=0.1 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn==0.13.0) (0.1.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas==2.2.3) (1.17.0)\n",
            "Downloading pandas-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m118.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading matplotlib-3.10.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m134.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading imbalanced_learn-0.13.0-py3-none-any.whl (238 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m238.4/238.4 kB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.15.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (37.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m37.6/37.6 MB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: scipy, pandas, matplotlib, imbalanced-learn\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.9.3\n",
            "    Uninstalling scipy-1.9.3:\n",
            "      Successfully uninstalled scipy-1.9.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "dopamine-rl 4.1.2 requires flax>=0.2.0, which is not installed.\n",
            "tensorflow-federated 0.84.0 requires scipy~=1.9.3, but you have scipy 1.15.2 which is incompatible.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\n",
            "google-colab 1.0.0 requires portpicker==1.5.2, but you have portpicker 1.6.0 which is incompatible.\n",
            "xarray 2025.1.2 requires packaging>=23.2, but you have packaging 22.0 which is incompatible.\n",
            "tensorflow-decision-forests 1.11.0 requires tensorflow==2.18.0, but you have tensorflow 2.14.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed imbalanced-learn-0.13.0 matplotlib-3.10.1 pandas-2.2.3 scipy-1.15.2\n",
            "Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.11/dist-packages (0.13.0)\n",
            "Requirement already satisfied: numpy<3,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (1.25.2)\n",
            "Requirement already satisfied: scipy<2,>=1.10.1 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (1.15.2)\n",
            "Requirement already satisfied: scikit-learn<2,>=1.3.2 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (1.6.1)\n",
            "Requirement already satisfied: sklearn-compat<1,>=0.1 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (0.1.3)\n",
            "Requirement already satisfied: joblib<2,>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl<4,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (3.6.0)\n",
            "Collecting faker\n",
            "  Downloading faker-37.1.0-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: tzdata in /usr/local/lib/python3.11/dist-packages (from faker) (2025.2)\n",
            "Downloading faker-37.1.0-py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faker\n",
            "Successfully installed faker-37.1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:jax._src.xla_bridge:Jax plugin configuration error: Plugin module %s could not be loaded\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/jax/_src/xla_bridge.py\", line 428, in discover_pjrt_plugins\n",
            "    plugin_module = importlib.import_module(plugin_module_name)\n",
            "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/importlib/__init__.py\", line 126, in import_module\n",
            "    return _bootstrap._gcd_import(name[level:], package, level)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<frozen importlib._bootstrap>\", line 1204, in _gcd_import\n",
            "  File \"<frozen importlib._bootstrap>\", line 1176, in _find_and_load\n",
            "  File \"<frozen importlib._bootstrap>\", line 1147, in _find_and_load_unlocked\n",
            "  File \"<frozen importlib._bootstrap>\", line 690, in _load_unlocked\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 940, in exec_module\n",
            "  File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/jax_plugins/xla_cuda12/__init__.py\", line 21, in <module>\n",
            "    from jax._src.lib import triton\n",
            "ImportError: cannot import name 'triton' from 'jax._src.lib' (/usr/local/lib/python3.11/dist-packages/jax/_src/lib/__init__.py)\n",
            "/usr/local/lib/python3.11/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
            "\n",
            "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
            "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
            "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
            "\n",
            "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
            "\n",
            "  warnings.warn(\n",
            "Exception ignored on calling ctypes callback function: <function ThreadpoolController._find_libraries_with_dl_iterate_phdr.<locals>.match_library_callback at 0x7d2ac9916020>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/threadpoolctl.py\", line 1005, in match_library_callback\n",
            "    self._make_controller_from_path(filepath)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/threadpoolctl.py\", line 1187, in _make_controller_from_path\n",
            "    lib_controller = controller_class(\n",
            "                     ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/threadpoolctl.py\", line 114, in __init__\n",
            "    self.dynlib = ctypes.CDLL(filepath, mode=_RTLD_NOLOAD)\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/ctypes/__init__.py\", line 376, in __init__\n",
            "    self._handle = _dlopen(self._name, mode)\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "OSError: dlopen() error\n"
          ]
        }
      ],
      "source": [
        "# Remove potential problematic packages\n",
        "!ls -la /usr/local/lib/python3.11/dist-packages | grep cipy\n",
        "\n",
        "# Remove the corrupted packages\n",
        "!rm -rf /usr/local/lib/python3.11/dist-packages/~cipy*\n",
        "\n",
        "# Uninstall problematic versions\n",
        "!pip uninstall -y jax jaxlib flax optax orbax-checkpoint ml-dtypes scipy tensorflow tensorflow-federated chex numpy pandas matplotlib scikit-learn imbalanced-learn\n",
        "\n",
        "# Install packages with correct versions\n",
        "!pip install tensorflow==2.14.0 tensorflow-federated==0.84.0 tensorflow-addons==0.22.0 tensorflow-privacy==0.9.0 scipy==1.9.3 ml-dtypes==0.2.0\n",
        "!pip install jax==0.4.27 jaxlib==0.4.27 flax==0.7.2 optax==0.1.7 orbax-checkpoint==0.11.10\n",
        "!pip install numpy==1.25.2 pandas==2.2.3 matplotlib==3.10.1 scikit-learn==1.6.1 imbalanced-learn==0.13.0\n",
        "\n",
        "!pip install -U imbalanced-learn\n",
        "\n",
        "!pip install faker\n",
        "\n",
        "# Imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import tensorflow_federated as tff\n",
        "import tensorflow_addons as tfa\n",
        "import tensorflow_privacy as tfp\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from collections import Counter\n",
        "\n",
        "from tensorflow_federated import aggregators\n",
        "from tensorflow_federated.python.aggregators import clipping_factory\n",
        "from tensorflow_federated.python.aggregators import zeroing_factory\n",
        "from tensorflow_privacy.privacy.dp_query import gaussian_query\n",
        "\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.over_sampling import BorderlineSMOTE\n",
        "\n",
        "from faker import Faker\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "import os\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive (used for dataset storage during model)\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# Store csv in dataframe\n",
        "x_ray_info = pd.read_csv('/content/drive/My Drive/your_dataset_folder/your_file.csv', header=0)  # <-- Replace with actual path"
      ],
      "metadata": {
        "id": "J3t6Nk3nb3f5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7964894-3bf0-42d8-d93a-362d99b8a55e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "IMAGE_SIZE       = (224, 224)\n",
        "BATCH_SIZE       = 16           # Smaller batch size for CPU\n",
        "AUTOTUNE         = tf.data.AUTOTUNE\n",
        "CLIENT_AMOUNT    = 4\n",
        "PROX             = 0.05         # Proximal strength for FedProx if used\n",
        "NUM_ROUNDS       = 50           # Federated rounds\n",
        "PATIENCE         = 8            # Patience FOR early stopping used\n",
        "CLIENT_LR        = 0.001        # Lower LR for CPU stability\n",
        "SERVER_LR        = 0.3          # Moderate server LR with momentum\n",
        "EPOCHS           = 1            # More epochs for CPU stability\n",
        "NOISE_MULTIPLIER = 0.02         # Gaussian noise multiplyer\n",
        "L2_NORM_CLIP     = 2.0          # Ensures adding noise will not spike\n",
        "MIN_DELTA        = 0.001        # Minimum amount of change\n",
        "\n",
        "INITIAL_LR       = 0.1          # unused\n",
        "WARM_UP          = 500\n",
        "TOTAL_STEPS      = 800\n",
        "DECAY_STEPS      = 800\n",
        "DECAY_RATE = 0.96\n",
        "\n",
        "\n",
        "LABEL_MAP        = {'Normal': 0, 'Pnemonia': 1}"
      ],
      "metadata": {
        "id": "Mn8wtrExcGNy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Path to folder with images\n",
        "image_folder_train = '/content/drive/My Drive/CS work/uni/year 3/3094 (dis)/coding/Complex_Dataset/train'\n",
        "image_folder_test = '/content/drive/My Drive/CS work/uni/year 3/3094 (dis)/coding/Complex_Dataset/test'"
      ],
      "metadata": {
        "id": "eCrCdt0EcO5_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_image_count = 0\n",
        "test_image_count = 0\n",
        "null_count = 0\n",
        "both_count = 0\n",
        "\n",
        "# Iterate through each row in the DataFrame\n",
        "for index, row in x_ray_info.iterrows():\n",
        "    image_name = row['X_ray_image_name']\n",
        "    image_path_train = os.path.join(image_folder_train, image_name)\n",
        "    image_path_test = os.path.join(image_folder_test, image_name)\n",
        "\n",
        "    # Load the image\n",
        "    if os.path.exists(image_path_train) and os.path.exists(image_path_test): # Validation\n",
        "        both_count += 1\n",
        "    elif os.path.exists(image_path_train): # Differentiate between train and Test data\n",
        "        train_image_count += 1\n",
        "        x_ray_info.at[index, 'in_data'] = \"Train\"\n",
        "    elif os.path.exists(image_path_test):\n",
        "        test_image_count += 1\n",
        "        x_ray_info.at[index, 'in_data'] = \"Test\"\n",
        "    else:\n",
        "        null_count += 1\n",
        "\n",
        "print(\"there are \" + str(train_image_count) + \" images in train folder\")\n",
        "print(\"there are \" + str(test_image_count) + \" images in test folder\")\n",
        "print(\"there are \" + str(null_count) + \" images not being used\")\n",
        "print(\"there are \" + str(both_count) + \" images in both folders\")\n",
        "\n",
        "print(\"-\"*40)\n",
        "\n",
        "# Drop all the data that was not stored in the drive\n",
        "x_ray_info = x_ray_info[x_ray_info['in_data'] != \"\"].reset_index(drop=True)\n",
        "\n",
        "print(x_ray_info['Label'].value_counts())\n",
        "\n",
        "print(\"-\"*40)"
      ],
      "metadata": {
        "id": "Cvwp0rglcgy0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3655987-b14f-413b-b26c-5fb0b76f3047"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "there are 3049 images in train folder\n",
            "there are 338 images in test folder\n",
            "there are 2523 images not being used\n",
            "there are 0 images in both folders\n",
            "----------------------------------------\n",
            "Label\n",
            "Pnemonia    4334\n",
            "Normal      1576\n",
            "Name: count, dtype: int64\n",
            "----------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get rid of unused data\n",
        "x_ray_info = x_ray_info.drop(columns=['Label_1_Virus_category', 'Label_2_Virus_category'])\n",
        "\n",
        "# Filter only the training data (since test data should not be modified)\n",
        "train_data = x_ray_info[x_ray_info['in_data'] == \"Train\"]\n",
        "\n",
        "# Separate into Normal and Pneumonia classes\n",
        "normal_samples = train_data[train_data['Label'] == 'Normal']\n",
        "pneumonia_samples = train_data[train_data['Label'] == 'Pnemonia']  # Match your label spelling\n",
        "\n",
        "# Randomly select 1000 from each (if available)\n",
        "selected_normal = normal_samples.sample(n=min(800, len(normal_samples)), random_state=42)\n",
        "selected_pneumonia = pneumonia_samples.sample(n=min(800, len(pneumonia_samples)), random_state=42)\n",
        "\n",
        "# Combine into a new balanced DataFrame\n",
        "balanced_df = pd.concat([selected_normal, selected_pneumonia]).reset_index(drop=True)\n",
        "\n",
        "# Verify the counts\n",
        "print(balanced_df['Label'].value_counts())\n",
        "print(\"-\"*40)\n",
        "print(f\"\\nNew balanced DataFrame shape: {balanced_df.shape}\")"
      ],
      "metadata": {
        "id": "B0Uu0YSUcm6t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15003fc6-72ae-4ce5-f5ca-25b56bfe3a6b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label\n",
            "Normal      800\n",
            "Pnemonia    800\n",
            "Name: count, dtype: int64\n",
            "----------------------------------------\n",
            "\n",
            "New balanced DataFrame shape: (1600, 5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create pseudo meta data for patients\n",
        "fake = Faker()\n",
        "balanced_df[\"age\"] = [fake.random_int(min=18, max=80) for _ in range(len(balanced_df))] #10 digit code\n",
        "balanced_df[\"patient_name\"] = [fake.name() for _ in range(len(balanced_df))]\n",
        "\n",
        "balanced_df.info()"
      ],
      "metadata": {
        "id": "vNaMzxYqNJ9m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd8af9c3-bd01-4c98-a7b9-bd2d23101a73"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1600 entries, 0 to 1599\n",
            "Data columns (total 7 columns):\n",
            " #   Column            Non-Null Count  Dtype \n",
            "---  ------            --------------  ----- \n",
            " 0   Unnamed: 0        1600 non-null   int64 \n",
            " 1   X_ray_image_name  1600 non-null   object\n",
            " 2   Label             1600 non-null   object\n",
            " 3   Dataset_type      1600 non-null   object\n",
            " 4   in_data           1600 non-null   object\n",
            " 5   age               1600 non-null   int64 \n",
            " 6   patient_name      1600 non-null   object\n",
            "dtypes: int64(2), object(5)\n",
            "memory usage: 87.6+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " # Pseudonymisation\n",
        " # Pseudonymise sensitive data by replacing personally identifiable information\n",
        "\n",
        "patient_data = {} # empty dictionary\n",
        "\n",
        "# Iterate through the dataFrame and populate the dictionary\n",
        "for index, row in balanced_df.iterrows():\n",
        "\n",
        "    # Generate a unique patient ID\n",
        "    patient_id = fake.random_int(min=1000000000, max=9999999999) # 10 digit code\n",
        "\n",
        "    while patient_id in patient_data:  # Ensure uniqueness\n",
        "        patient_id = fake.random_int(min=1000000000, max=9999999999)\n",
        "\n",
        "    # Add the patient name and ID to the dictionary\n",
        "    patient_data[row['patient_name']] = patient_id\n",
        "\n",
        "# Check\n",
        "for i in range(min(3, len(patient_data))):\n",
        "    key = list(patient_data.keys())[i]  # Get the key at index i\n",
        "    value = patient_data[key]  # Get the value associated with the key\n",
        "    print(f\"Patient: {key}, ID: {value}\")\n",
        "\n",
        "balanced_df[\"patient_name\"] = balanced_df[\"patient_name\"].map(patient_data)\n",
        "print(\"-\"*40)\n",
        "balanced_df[\"patient_name\"].values"
      ],
      "metadata": {
        "id": "i-8oiDGzYJrg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7330af9b-42f2-40e8-a633-78049dead829"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Patient: Charles Wallace, ID: 6655802004\n",
            "Patient: Marco Norton, ID: 9647342437\n",
            "Patient: Ashley Blanchard, ID: 7706561040\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([6655802004, 9647342437, 7706561040, ..., 5838400377, 9648292850,\n",
              "       9601499362])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generalisation\n",
        "# To generalise data apply transformations to discrete values and put them into catagories\n",
        "\n",
        "patient_age_data = {}\n",
        "\n",
        "# Define age groups\n",
        "age_groups = {\n",
        "    '18-30': [],\n",
        "    '31-40': [],\n",
        "    '41-50': [],\n",
        "    '51-60': [],\n",
        "    '61+': []\n",
        "}\n",
        "\n",
        "# Function to categorize ages\n",
        "def generalize_age(age):\n",
        "    if 18 <= age <= 30:\n",
        "        return '18-30'\n",
        "    elif 31 <= age <= 40:\n",
        "        return '31-40'\n",
        "    elif 41 <= age <= 50:\n",
        "        return '41-50'\n",
        "    elif 51 <= age <= 60:\n",
        "        return '51-60'\n",
        "    else:\n",
        "        return '61+'\n",
        "\n",
        "# Apply generalisation to DataFrame\n",
        "balanced_df['age_group'] = balanced_df['age'].apply(generalize_age)\n",
        "\n",
        "# Store original ages\n",
        "for index, row in balanced_df.iterrows():\n",
        "    patient_age_data[row['patient_name']] = row['age']\n",
        "\n",
        "print(balanced_df[['patient_name', 'age']].head(3))\n",
        "balanced_df = balanced_df.drop(columns=['age'])\n",
        "print(\"-\"*40)\n",
        "print(len(balanced_df['age_group']))\n",
        "print(\"-\"*40)\n",
        "print(balanced_df[['patient_name', 'age_group']].head(3))"
      ],
      "metadata": {
        "id": "ZYkq8mj4gJNq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a587f818-e87d-4623-b23f-0552ea30c6c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   patient_name  age\n",
            "0    6655802004   44\n",
            "1    9647342437   21\n",
            "2    7706561040   38\n",
            "----------------------------------------\n",
            "1600\n",
            "----------------------------------------\n",
            "   patient_name age_group\n",
            "0    6655802004     41-50\n",
            "1    9647342437     18-30\n",
            "2    7706561040     31-40\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_and_preprocess(path):\n",
        "    img = tf.io.read_file(path)\n",
        "    img = tf.image.decode_jpeg(img, channels=1) # Decode JPEG-encoded image to a single grayscale channel\n",
        "    img = tf.image.resize(img, IMAGE_SIZE) # Resize image\n",
        "    return tf.cast(img, tf.float32) / 255.0 # Normalise\n",
        "\n",
        "def parse_image(path, label):\n",
        "    img = tf.io.read_file(path)\n",
        "    img = tf.image.decode_jpeg(img, channels=1)\n",
        "    img = tf.image.resize(img, IMAGE_SIZE)\n",
        "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
        "    return img, label\n",
        "\n",
        "# randomly alter the pictures so the model will focus on the essential features\n",
        "def augment_image(img, label):\n",
        "    img = tf.image.random_flip_left_right(img) # Random Horizontal flip\n",
        "    img = tf.image.random_brightness(img, max_delta=0.1) # Random brightness adjustment\n",
        "    img = tf.image.random_contrast(img, lower=0.9, upper=1.1) # Random contrast adjustment\n",
        "    # small rotation ±10° for variance\n",
        "    angle = tf.random.uniform([], -0.174, 0.174)\n",
        "    img = tfa.image.rotate(img, angle)\n",
        "    return img, label\n",
        "\n",
        "def extract_path_and_label(row):\n",
        "    image_name = row['X_ray_image_name'] # Find image name\n",
        "    label = LABEL_MAP[row['Label']] # Extract if it's label\n",
        "    if row['in_data'] == \"Train\":\n",
        "        path = os.path.join(image_folder_train, image_name) # Create correct path by adding image_name to train data path\n",
        "    else:\n",
        "        path = os.path.join(image_folder_test, image_name) # Incase of error\n",
        "    return path, label"
      ],
      "metadata": {
        "id": "l26MD7-Ic7dE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_federated_datasets(df, client_count, epochs, augment):\n",
        "\n",
        "    # Shuffle and split indices across clients\n",
        "    indices = df.index.tolist()\n",
        "    np.random.shuffle(indices)\n",
        "    client_indices = np.array_split(indices, client_count)\n",
        "    client_data = []\n",
        "\n",
        "    for indices in client_indices:\n",
        "        # Gather file paths and numeric labels for this client\n",
        "        client_rows = df.loc[indices].reset_index(drop=True)\n",
        "        records = client_rows.to_dict(orient='records')\n",
        "        image_paths = []\n",
        "        labels = []\n",
        "\n",
        "        for row in records:\n",
        "            path, label = extract_path_and_label(row)\n",
        "            if os.path.exists(path):\n",
        "                image_paths.append(path)\n",
        "                labels.append(label)\n",
        "\n",
        "        path_ds = tf.data.Dataset.from_tensor_slices(image_paths)\n",
        "        label_ds = tf.data.Dataset.from_tensor_slices(labels)\n",
        "        ds = tf.data.Dataset.zip((path_ds, label_ds))\n",
        "\n",
        "        # Decode and process the image\n",
        "        ds = ds.map(parse_image, num_parallel_calls=AUTOTUNE)\n",
        "\n",
        "        # Potentially augment data\n",
        "        if augment:\n",
        "            ds = ds.map(augment_image, num_parallel_calls=AUTOTUNE)\n",
        "\n",
        "        ds = ds.cache() # Speeds up subsequent epochs by avoiding repeated disk\n",
        "        ds = ds.repeat(epochs)\n",
        "        ds = ds.shuffle(buffer_size=len(image_paths)) # Improve generalisation\n",
        "        ds = ds.batch(BATCH_SIZE) # Group examples into batches\n",
        "        ds = ds.prefetch(AUTOTUNE) # Maximises efficiency by preloading next batch while training a current batch\n",
        "\n",
        "        client_data.append(ds)\n",
        "\n",
        "        # Report per-client statistics\n",
        "        print(f\"Client {len(client_data)} has {len(image_paths)} images\")\n",
        "        print(f\"Labels distribution: {Counter(labels)}\")\n",
        "\n",
        "    return client_data"
      ],
      "metadata": {
        "id": "mpDryyrudRaT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_keras_model_comlex(): #(complex isn't as effective)\n",
        "\n",
        "    # Use He normal initialiser for improved training stability with ReLU-like activations\n",
        "    initializer = tf.keras.initializers.HeNormal()\n",
        "\n",
        "    # Mitigate overfitting by adding weight decay\n",
        "    l2_regularizer = tf.keras.regularizers.L2(1e-4)\n",
        "\n",
        "    model = tf.keras.Sequential([\n",
        "\n",
        "        # Define input\n",
        "        tf.keras.layers.Input(shape=(224, 224, 1)),\n",
        "\n",
        "        # First convolutional block 32 filters\n",
        "        # Swish activation provides smoother gradients\n",
        "        tf.keras.layers.Conv2D(32, 3, padding='same',\n",
        "                               kernel_initializer=initializer,\n",
        "                               kernel_regularizer=l2_regularizer,\n",
        "                               use_bias=False),\n",
        "        tfa.layers.GroupNormalization(groups=8, axis=-1, epsilon=1e-5),\n",
        "        tf.keras.layers.Activation('swish'),\n",
        "        tf.keras.layers.MaxPooling2D(),\n",
        "\n",
        "        # Second convolutional block 64 filters\n",
        "        tf.keras.layers.SeparableConv2D(64, 3, padding='same',\n",
        "                                        kernel_regularizer=l2_regularizer,\n",
        "                                        use_bias=False),\n",
        "        tfa.layers.GroupNormalization(groups=8, axis=-1, epsilon=1e-5),\n",
        "        tf.keras.layers.Activation('swish'),\n",
        "         tf.keras.layers.SpatialDropout2D(0.15),\n",
        "        tf.keras.layers.MaxPooling2D(),\n",
        "\n",
        "        # Third convolutional block 128 filters\n",
        "        tf.keras.layers.SeparableConv2D(128, 3, padding='same',\n",
        "                                        kernel_regularizer=l2_regularizer,\n",
        "                                        use_bias=False),\n",
        "        tfa.layers.GroupNormalization(groups=8, axis=-1, epsilon=1e-5),\n",
        "        tf.keras.layers.Activation('swish'),\n",
        "         tf.keras.layers.SpatialDropout2D(0.15),\n",
        "        tf.keras.layers.MaxPooling2D(),\n",
        "\n",
        "        # Block 4\n",
        "        tf.keras.layers.SeparableConv2D(256, 3, padding='same',\n",
        "                                        kernel_regularizer=l2_regularizer,\n",
        "                                        use_bias=False),\n",
        "        tfa.layers.GroupNormalization(groups=8, axis=-1, epsilon=1e-5),\n",
        "        tf.keras.layers.Activation('swish'),\n",
        "         tf.keras.layers.SpatialDropout2D(0.15),\n",
        "        tf.keras.layers.MaxPooling2D(),\n",
        "\n",
        "        # Pool + Dense head\n",
        "        tf.keras.layers.GlobalAveragePooling2D(),\n",
        "        tf.keras.layers.Dense(128,\n",
        "                              kernel_regularizer=l2_regularizer,\n",
        "                              use_bias=False),\n",
        "        tfa.layers.GroupNormalization(groups=4, axis=-1, epsilon=1e-5),\n",
        "        tf.keras.layers.Activation('swish'),\n",
        "        tf.keras.layers.Dropout(0.7),\n",
        "\n",
        "        # Use float32 for final output to prevent dtype mismatch with loss functions\n",
        "        tf.keras.layers.Dense(1, activation='sigmoid', dtype='float32')\n",
        "    ])\n",
        "    return model"
      ],
      "metadata": {
        "id": "fgWiu0YQdbr2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_keras_model(): #(simple)\n",
        "    initializer = tf.keras.initializers.HeNormal()\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Input(shape=(224, 224, 1)),\n",
        "\n",
        "        # First convolutional block 32 filters\n",
        "        # Swish activation provides smoother gradients\n",
        "        tf.keras.layers.Conv2D(32, (3, 3), activation='swish', padding='same', kernel_initializer=initializer),\n",
        "        tf.keras.layers.LayerNormalization(),\n",
        "        tf.keras.layers.MaxPooling2D(),\n",
        "\n",
        "        # Second convolutional block 64 filters\n",
        "        tf.keras.layers.SeparableConv2D(64, (3, 3), activation='swish', padding='same'),\n",
        "        tf.keras.layers.LayerNormalization(),\n",
        "        tf.keras.layers.MaxPooling2D(),\n",
        "\n",
        "        # Third convolutional block 128 filters\n",
        "        tf.keras.layers.SeparableConv2D(128, (3, 3), activation='swish', padding='same'),\n",
        "        tf.keras.layers.LayerNormalization(),\n",
        "        tf.keras.layers.MaxPooling2D(),\n",
        "\n",
        "        # Fourth convolutional block 256 filters\n",
        "        tf.keras.layers.SeparableConv2D(256, (3, 3), activation='swish', padding='same'),\n",
        "        tf.keras.layers.LayerNormalization(),\n",
        "        tf.keras.layers.MaxPooling2D(),\n",
        "\n",
        "        # Pool + Dense head\n",
        "        tf.keras.layers.GlobalAveragePooling2D(),\n",
        "        tf.keras.layers.Dense(128, activation='swish'),\n",
        "        tf.keras.layers.Dropout(0.5),\n",
        "\n",
        "        # Use float32 for final output to prevent dtype mismatch with loss functions\n",
        "        tf.keras.layers.Dense(1, activation='sigmoid', dtype='float32')\n",
        "    ])\n",
        "    return model"
      ],
      "metadata": {
        "id": "aQ0tLoqtfy0w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def model_fn():\n",
        "    keras_model = create_keras_model()\n",
        "\n",
        "    return tff.learning.models.from_keras_model(\n",
        "        keras_model,\n",
        "        input_spec=(\n",
        "            tf.TensorSpec(shape=(None, 224, 224, 1), dtype=tf.float32),  # Batch of images\n",
        "            tf.TensorSpec(shape=(None), dtype=tf.int32)               # Batch of labels\n",
        "        ),\n",
        "        loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "        metrics=[\n",
        "            tf.keras.metrics.BinaryAccuracy(),\n",
        "            tf.keras.metrics.Precision(name='precision'), # Precision of positive class\n",
        "            tf.keras.metrics.Recall(name='recall'), # Sensitivity/recall of positives\n",
        "            tf.keras.metrics.AUC(name='auc') # Area under ROC curve\n",
        "        ]\n",
        "    )"
      ],
      "metadata": {
        "id": "zru1B_SReUPl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Differential Privacy\n",
        "\n",
        "# This process provides formal differential privacy guarantees\n",
        "# The presence or absence of any individual client barely changes the outcome of training, making it provably private\n",
        "dp_agg = tff.aggregators.DifferentiallyPrivateFactory.gaussian_fixed(\n",
        "    noise_multiplier=NOISE_MULTIPLIER,\n",
        "    clients_per_round=CLIENT_AMOUNT, # Number of clients participating in each training round\n",
        "    clip=L2_NORM_CLIP\n",
        ")\n",
        "\n",
        "weighted_dp_agg = tff.aggregators.as_weighted_aggregator(dp_agg)"
      ],
      "metadata": {
        "id": "Nj_ae-YT90UG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "iterative_process = tff.learning.algorithms.build_weighted_fed_avg(\n",
        "    # Model function\n",
        "    model_fn=model_fn,\n",
        "\n",
        "    # Optimiser used by clients for local training\n",
        "    client_optimizer_fn=lambda: tf.keras.optimizers.SGD(\n",
        "        learning_rate=CLIENT_LR, momentum=0.9,\n",
        "    ),\n",
        "\n",
        "    # Optimiser used by the server to update the global model\n",
        "    server_optimizer_fn=lambda: tf.keras.optimizers.SGD(\n",
        "        learning_rate=SERVER_LR, momentum=0.9,\n",
        "    ),\n",
        "    # Aggregation strategy that enforces differential privacy\n",
        "    model_aggregator=None\n",
        ")\n",
        "\n",
        "train_data = create_federated_datasets(balanced_df, CLIENT_AMOUNT, EPOCHS, True)\n",
        "state = iterative_process.initialize()\n"
      ],
      "metadata": {
        "id": "I5K5kwdDeA2v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "948eee5f-5a0c-4470-f3c1-45f8d4ae7612"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Client 1 has 400 images\n",
            "Labels distribution: Counter({1: 206, 0: 194})\n",
            "Client 2 has 400 images\n",
            "Labels distribution: Counter({0: 204, 1: 196})\n",
            "Client 3 has 400 images\n",
            "Labels distribution: Counter({0: 208, 1: 192})\n",
            "Client 4 has 400 images\n",
            "Labels distribution: Counter({1: 206, 0: 194})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Store metrics\n",
        "metrics_history = []\n",
        "best_auc = 0.0\n",
        "wait = 0\n",
        "\n",
        "for round_num in range(1, NUM_ROUNDS + 1):\n",
        "    state, metrics = iterative_process.next(state, train_data)\n",
        "\n",
        "    # Extract metrics\n",
        "    train_metrics = metrics['client_work']['train']\n",
        "    loss = train_metrics['loss']\n",
        "    acc = train_metrics['binary_accuracy']\n",
        "    prec = train_metrics['precision']\n",
        "    rec = train_metrics['recall']\n",
        "    auc = train_metrics['auc']\n",
        "\n",
        "        # Save metrics to list\n",
        "    metrics_history.append({\n",
        "        'round': round_num,\n",
        "        'loss': loss,\n",
        "        'accuracy': acc,\n",
        "        'precision': prec,\n",
        "        'recall': rec,\n",
        "        'auc': auc\n",
        "    })\n",
        "\n",
        "    # Formatted output\n",
        "    print(f\"\\nRound {round_num}\")\n",
        "    print(\"-\"*40)\n",
        "    print(f\"Train Metrics:\")\n",
        "    print(f\"Loss:     {loss:.4f}\")\n",
        "    print(f\"Accuracy: {acc:.4f}\")\n",
        "    print(f\"Precision:{prec:.4f}\")\n",
        "    print(f\"Recall:   {rec:.4f}\")\n",
        "    print(f\"AUC:      {auc:.4f}\")\n",
        "\n",
        "    # Early stopping condition based on AUC\n",
        "    if auc > best_auc + MIN_DELTA:\n",
        "        best_auc = auc\n",
        "        wait = 0\n",
        "    else:\n",
        "        wait += 1\n",
        "        print(f\"Early stopping counter: {wait}/{PATIENCE}\")\n",
        "        if wait >= PATIENCE:\n",
        "            print(\"Early stopping triggered.\")\n",
        "            break"
      ],
      "metadata": {
        "id": "5PTRqdJpfWL3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c621c9bb-b9b7-418a-df3c-940f799c82b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Round 1\n",
            "----------------------------------------\n",
            "Train Metrics:\n",
            "Loss:     0.7370\n",
            "Accuracy: 0.5225\n",
            "Precision:0.5221\n",
            "Recall:   0.5312\n",
            "AUC:      0.5179\n",
            "\n",
            "Round 2\n",
            "----------------------------------------\n",
            "Train Metrics:\n",
            "Loss:     0.7263\n",
            "Accuracy: 0.5150\n",
            "Precision:0.5146\n",
            "Recall:   0.5275\n",
            "AUC:      0.5211\n",
            "\n",
            "Round 3\n",
            "----------------------------------------\n",
            "Train Metrics:\n",
            "Loss:     0.7083\n",
            "Accuracy: 0.5200\n",
            "Precision:0.5196\n",
            "Recall:   0.5300\n",
            "AUC:      0.5299\n",
            "\n",
            "Round 4\n",
            "----------------------------------------\n",
            "Train Metrics:\n",
            "Loss:     0.7023\n",
            "Accuracy: 0.5156\n",
            "Precision:0.5175\n",
            "Recall:   0.4625\n",
            "AUC:      0.5392\n",
            "\n",
            "Round 5\n",
            "----------------------------------------\n",
            "Train Metrics:\n",
            "Loss:     0.6949\n",
            "Accuracy: 0.5369\n",
            "Precision:0.5369\n",
            "Recall:   0.5362\n",
            "AUC:      0.5598\n",
            "\n",
            "Round 6\n",
            "----------------------------------------\n",
            "Train Metrics:\n",
            "Loss:     0.6941\n",
            "Accuracy: 0.5500\n",
            "Precision:0.5532\n",
            "Recall:   0.5200\n",
            "AUC:      0.5617\n",
            "\n",
            "Round 7\n",
            "----------------------------------------\n",
            "Train Metrics:\n",
            "Loss:     0.6883\n",
            "Accuracy: 0.5569\n",
            "Precision:0.5589\n",
            "Recall:   0.5400\n",
            "AUC:      0.5723\n",
            "\n",
            "Round 8\n",
            "----------------------------------------\n",
            "Train Metrics:\n",
            "Loss:     0.6860\n",
            "Accuracy: 0.5656\n",
            "Precision:0.5620\n",
            "Recall:   0.5950\n",
            "AUC:      0.5875\n",
            "\n",
            "Round 9\n",
            "----------------------------------------\n",
            "Train Metrics:\n",
            "Loss:     0.6836\n",
            "Accuracy: 0.5688\n",
            "Precision:0.5655\n",
            "Recall:   0.5938\n",
            "AUC:      0.5925\n",
            "\n",
            "Round 10\n",
            "----------------------------------------\n",
            "Train Metrics:\n",
            "Loss:     0.6749\n",
            "Accuracy: 0.5725\n",
            "Precision:0.5658\n",
            "Recall:   0.6237\n",
            "AUC:      0.6091\n",
            "\n",
            "Round 11\n",
            "----------------------------------------\n",
            "Train Metrics:\n",
            "Loss:     0.6738\n",
            "Accuracy: 0.5713\n",
            "Precision:0.5658\n",
            "Recall:   0.6125\n",
            "AUC:      0.6138\n",
            "\n",
            "Round 12\n",
            "----------------------------------------\n",
            "Train Metrics:\n",
            "Loss:     0.6723\n",
            "Accuracy: 0.5888\n",
            "Precision:0.5885\n",
            "Recall:   0.5900\n",
            "AUC:      0.6139\n",
            "Early stopping counter: 1/8\n",
            "\n",
            "Round 13\n",
            "----------------------------------------\n",
            "Train Metrics:\n",
            "Loss:     0.6811\n",
            "Accuracy: 0.5869\n",
            "Precision:0.5821\n",
            "Recall:   0.6162\n",
            "AUC:      0.6073\n",
            "Early stopping counter: 2/8\n",
            "\n",
            "Round 14\n",
            "----------------------------------------\n",
            "Train Metrics:\n",
            "Loss:     0.6764\n",
            "Accuracy: 0.5869\n",
            "Precision:0.5879\n",
            "Recall:   0.5813\n",
            "AUC:      0.6180\n",
            "\n",
            "Round 15\n",
            "----------------------------------------\n",
            "Train Metrics:\n",
            "Loss:     0.6711\n",
            "Accuracy: 0.5931\n",
            "Precision:0.5961\n",
            "Recall:   0.5775\n",
            "AUC:      0.6260\n",
            "\n",
            "Round 16\n",
            "----------------------------------------\n",
            "Train Metrics:\n",
            "Loss:     0.6761\n",
            "Accuracy: 0.5744\n",
            "Precision:0.5723\n",
            "Recall:   0.5888\n",
            "AUC:      0.6171\n",
            "Early stopping counter: 1/8\n",
            "\n",
            "Round 17\n",
            "----------------------------------------\n",
            "Train Metrics:\n",
            "Loss:     0.6816\n",
            "Accuracy: 0.5900\n",
            "Precision:0.5847\n",
            "Recall:   0.6212\n",
            "AUC:      0.6142\n",
            "Early stopping counter: 2/8\n",
            "\n",
            "Round 18\n",
            "----------------------------------------\n",
            "Train Metrics:\n",
            "Loss:     0.6540\n",
            "Accuracy: 0.6169\n",
            "Precision:0.6125\n",
            "Recall:   0.6363\n",
            "AUC:      0.6547\n",
            "\n",
            "Round 19\n",
            "----------------------------------------\n",
            "Train Metrics:\n",
            "Loss:     0.6402\n",
            "Accuracy: 0.6419\n",
            "Precision:0.6356\n",
            "Recall:   0.6650\n",
            "AUC:      0.6865\n",
            "\n",
            "Round 20\n",
            "----------------------------------------\n",
            "Train Metrics:\n",
            "Loss:     0.6629\n",
            "Accuracy: 0.5950\n",
            "Precision:0.5913\n",
            "Recall:   0.6150\n",
            "AUC:      0.6467\n",
            "Early stopping counter: 1/8\n",
            "\n",
            "Round 21\n",
            "----------------------------------------\n",
            "Train Metrics:\n",
            "Loss:     0.6562\n",
            "Accuracy: 0.6150\n",
            "Precision:0.6127\n",
            "Recall:   0.6250\n",
            "AUC:      0.6599\n",
            "Early stopping counter: 2/8\n",
            "\n",
            "Round 22\n",
            "----------------------------------------\n",
            "Train Metrics:\n",
            "Loss:     0.6593\n",
            "Accuracy: 0.6031\n",
            "Precision:0.6067\n",
            "Recall:   0.5863\n",
            "AUC:      0.6596\n",
            "Early stopping counter: 3/8\n",
            "\n",
            "Round 23\n",
            "----------------------------------------\n",
            "Train Metrics:\n",
            "Loss:     0.6302\n",
            "Accuracy: 0.6494\n",
            "Precision:0.6473\n",
            "Recall:   0.6562\n",
            "AUC:      0.7006\n",
            "\n",
            "Round 24\n",
            "----------------------------------------\n",
            "Train Metrics:\n",
            "Loss:     0.6220\n",
            "Accuracy: 0.6619\n",
            "Precision:0.6570\n",
            "Recall:   0.6775\n",
            "AUC:      0.7168\n",
            "\n",
            "Round 25\n",
            "----------------------------------------\n",
            "Train Metrics:\n",
            "Loss:     0.6294\n",
            "Accuracy: 0.6612\n",
            "Precision:0.6608\n",
            "Recall:   0.6625\n",
            "AUC:      0.7069\n",
            "Early stopping counter: 1/8\n",
            "\n",
            "Round 26\n",
            "----------------------------------------\n",
            "Train Metrics:\n",
            "Loss:     0.5918\n",
            "Accuracy: 0.6856\n",
            "Precision:0.6787\n",
            "Recall:   0.7050\n",
            "AUC:      0.7503\n",
            "\n",
            "Round 27\n",
            "----------------------------------------\n",
            "Train Metrics:\n",
            "Loss:     0.6007\n",
            "Accuracy: 0.6769\n",
            "Precision:0.6826\n",
            "Recall:   0.6612\n",
            "AUC:      0.7469\n",
            "Early stopping counter: 1/8\n",
            "\n",
            "Round 28\n",
            "----------------------------------------\n",
            "Train Metrics:\n",
            "Loss:     0.5625\n",
            "Accuracy: 0.7125\n",
            "Precision:0.7267\n",
            "Recall:   0.6812\n",
            "AUC:      0.7869\n",
            "\n",
            "Round 29\n",
            "----------------------------------------\n",
            "Train Metrics:\n",
            "Loss:     0.5693\n",
            "Accuracy: 0.7175\n",
            "Precision:0.7242\n",
            "Recall:   0.7025\n",
            "AUC:      0.7760\n",
            "Early stopping counter: 1/8\n",
            "\n",
            "Round 30\n",
            "----------------------------------------\n",
            "Train Metrics:\n",
            "Loss:     0.5856\n",
            "Accuracy: 0.7038\n",
            "Precision:0.7095\n",
            "Recall:   0.6900\n",
            "AUC:      0.7612\n",
            "Early stopping counter: 2/8\n",
            "\n",
            "Round 31\n",
            "----------------------------------------\n",
            "Train Metrics:\n",
            "Loss:     0.5649\n",
            "Accuracy: 0.7244\n",
            "Precision:0.7422\n",
            "Recall:   0.6875\n",
            "AUC:      0.7817\n",
            "Early stopping counter: 3/8\n",
            "\n",
            "Round 32\n",
            "----------------------------------------\n",
            "Train Metrics:\n",
            "Loss:     0.5338\n",
            "Accuracy: 0.7394\n",
            "Precision:0.7663\n",
            "Recall:   0.6888\n",
            "AUC:      0.8074\n",
            "\n",
            "Round 33\n",
            "----------------------------------------\n",
            "Train Metrics:\n",
            "Loss:     0.5467\n",
            "Accuracy: 0.7163\n",
            "Precision:0.7390\n",
            "Recall:   0.6687\n",
            "AUC:      0.7918\n",
            "Early stopping counter: 1/8\n",
            "\n",
            "Round 34\n",
            "----------------------------------------\n",
            "Train Metrics:\n",
            "Loss:     0.5321\n",
            "Accuracy: 0.7487\n",
            "Precision:0.7734\n",
            "Recall:   0.7038\n",
            "AUC:      0.8106\n",
            "\n",
            "Round 35\n",
            "----------------------------------------\n",
            "Train Metrics:\n",
            "Loss:     0.5363\n",
            "Accuracy: 0.7356\n",
            "Precision:0.7497\n",
            "Recall:   0.7075\n",
            "AUC:      0.8027\n",
            "Early stopping counter: 1/8\n",
            "\n",
            "Round 36\n",
            "----------------------------------------\n",
            "Train Metrics:\n",
            "Loss:     0.5292\n",
            "Accuracy: 0.7412\n",
            "Precision:0.7765\n",
            "Recall:   0.6775\n",
            "AUC:      0.8131\n",
            "\n",
            "Round 37\n",
            "----------------------------------------\n",
            "Train Metrics:\n",
            "Loss:     0.5167\n",
            "Accuracy: 0.7500\n",
            "Precision:0.7817\n",
            "Recall:   0.6938\n",
            "AUC:      0.8191\n",
            "\n",
            "Round 38\n",
            "----------------------------------------\n",
            "Train Metrics:\n",
            "Loss:     0.5320\n",
            "Accuracy: 0.7344\n",
            "Precision:0.7682\n",
            "Recall:   0.6712\n",
            "AUC:      0.8065\n",
            "Early stopping counter: 1/8\n",
            "\n",
            "Round 39\n",
            "----------------------------------------\n",
            "Train Metrics:\n",
            "Loss:     0.4889\n",
            "Accuracy: 0.7613\n",
            "Precision:0.7944\n",
            "Recall:   0.7050\n",
            "AUC:      0.8393\n",
            "\n",
            "Round 40\n",
            "----------------------------------------\n",
            "Train Metrics:\n",
            "Loss:     0.4966\n",
            "Accuracy: 0.7619\n",
            "Precision:0.7922\n",
            "Recall:   0.7100\n",
            "AUC:      0.8336\n",
            "Early stopping counter: 1/8\n",
            "\n",
            "Round 41\n",
            "----------------------------------------\n",
            "Train Metrics:\n",
            "Loss:     0.4997\n",
            "Accuracy: 0.7638\n",
            "Precision:0.7963\n",
            "Recall:   0.7088\n",
            "AUC:      0.8341\n",
            "Early stopping counter: 2/8\n",
            "\n",
            "Round 42\n",
            "----------------------------------------\n",
            "Train Metrics:\n",
            "Loss:     0.4948\n",
            "Accuracy: 0.7631\n",
            "Precision:0.7986\n",
            "Recall:   0.7038\n",
            "AUC:      0.8327\n",
            "Early stopping counter: 3/8\n",
            "\n",
            "Round 43\n",
            "----------------------------------------\n",
            "Train Metrics:\n",
            "Loss:     0.4777\n",
            "Accuracy: 0.7806\n",
            "Precision:0.8221\n",
            "Recall:   0.7163\n",
            "AUC:      0.8457\n",
            "\n",
            "Round 44\n",
            "----------------------------------------\n",
            "Train Metrics:\n",
            "Loss:     0.4828\n",
            "Accuracy: 0.7650\n",
            "Precision:0.7986\n",
            "Recall:   0.7088\n",
            "AUC:      0.8439\n",
            "Early stopping counter: 1/8\n",
            "\n",
            "Round 45\n",
            "----------------------------------------\n",
            "Train Metrics:\n",
            "Loss:     0.4942\n",
            "Accuracy: 0.7556\n",
            "Precision:0.7909\n",
            "Recall:   0.6950\n",
            "AUC:      0.8319\n",
            "Early stopping counter: 2/8\n",
            "\n",
            "Round 46\n",
            "----------------------------------------\n",
            "Train Metrics:\n",
            "Loss:     0.4669\n",
            "Accuracy: 0.7819\n",
            "Precision:0.8245\n",
            "Recall:   0.7163\n",
            "AUC:      0.8544\n",
            "\n",
            "Round 47\n",
            "----------------------------------------\n",
            "Train Metrics:\n",
            "Loss:     0.4619\n",
            "Accuracy: 0.7906\n",
            "Precision:0.8444\n",
            "Recall:   0.7125\n",
            "AUC:      0.8570\n",
            "\n",
            "Round 48\n",
            "----------------------------------------\n",
            "Train Metrics:\n",
            "Loss:     0.4679\n",
            "Accuracy: 0.7975\n",
            "Precision:0.8296\n",
            "Recall:   0.7487\n",
            "AUC:      0.8549\n",
            "Early stopping counter: 1/8\n",
            "\n",
            "Round 49\n",
            "----------------------------------------\n",
            "Train Metrics:\n",
            "Loss:     0.4453\n",
            "Accuracy: 0.7962\n",
            "Precision:0.8357\n",
            "Recall:   0.7375\n",
            "AUC:      0.8667\n",
            "\n",
            "Round 50\n",
            "----------------------------------------\n",
            "Train Metrics:\n",
            "Loss:     0.4683\n",
            "Accuracy: 0.7837\n",
            "Precision:0.8252\n",
            "Recall:   0.7200\n",
            "AUC:      0.8574\n",
            "Early stopping counter: 1/8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Directory to save data\n",
        "directory = '/content/drive/My Drive/your_metric_folder/' # <-- Replace with actual path\n",
        "if not os.path.exists(directory):\n",
        "    os.makedirs(directory)\n",
        "\n",
        "# Save metrics to CSV\n",
        "metrics_df = pd.DataFrame(metrics_history)\n",
        "metrics_df.to_csv(os.path.join(directory, 'Complex_Model_with_dp(2).csv'), index=False) # Change depending on use of differential privacy\n",
        "\n"
      ],
      "metadata": {
        "id": "Blc7AbYQXqDo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract final model weights\n",
        "final_model_weights = iterative_process.get_model_weights(state)\n",
        "\n",
        "# Create and compile a fresh Keras model\n",
        "keras_eval_model = create_keras_model()\n",
        "\n",
        "# Compile it for evaluation\n",
        "keras_eval_model.compile(\n",
        "    loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "    optimizer=tf.keras.optimizers.SGD(),\n",
        "        metrics=[\n",
        "            tf.keras.metrics.BinaryAccuracy(),\n",
        "            tf.keras.metrics.Precision(name='precision'),\n",
        "            tf.keras.metrics.Recall(name='recall'),\n",
        "            tf.keras.metrics.AUC(name='auc')\n",
        "        ]\n",
        ")\n",
        "\n",
        "# Load in the federated-trained weights\n",
        "final_model_weights.assign_weights_to(keras_eval_model)\n",
        "\n",
        "\n",
        "# Isolate test data 'not balanced'\n",
        "test_df = x_ray_info[x_ray_info['in_data'] == 'Test'].reset_index(drop=True)\n",
        "\n",
        "# Apply preprocessing and create single dataset\n",
        "test_dataset = create_federated_datasets(test_df, 1, EPOCHS, True)[0]\n",
        "\n",
        "# Evaluate on the test data\n",
        "print(\"-\"*40)\n",
        "results = keras_eval_model.evaluate(test_dataset)\n",
        "print(f\"Loss:          {results[0]:.4f}\")\n",
        "print(f\"Accuracy:      {results[1]:.4f}\")\n",
        "print(f\"Precision:     {results[2]:.4f}\")\n",
        "print(f\"Recall:        {results[3]:.4f}\")\n",
        "print(f\"AUC:           {results[4]:.4f}\")\n",
        "\n",
        "# Create data frame to store test results\n",
        "results_df = pd.DataFrame({\n",
        "    'metric': ['Loss', 'Accuracy', 'Precision', 'Recall', 'AUC'],\n",
        "    'value': results\n",
        "})\n",
        "\n",
        "# Store test Results in csv\n",
        "results_df.to_csv(os.path.join(directory, 'test_results_with_dp(2).csv'), index=False) # Change depending on use of differential privacy\n"
      ],
      "metadata": {
        "id": "Fi8qnuS9Zjbh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "127fd994-d108-4395-ffe4-7fdefbed6e29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Client 1 has 338 images\n",
            "Labels distribution: Counter({0: 188, 1: 150})\n",
            "----------------------------------------\n",
            "22/22 [==============================] - 78s 835ms/step - loss: 0.6010 - binary_accuracy: 0.6834 - precision: 0.6215 - recall: 0.7333 - auc: 0.7809\n",
            "Loss:          0.6010\n",
            "Accuracy:      0.6834\n",
            "Precision:     0.6215\n",
            "Recall:        0.7333\n",
            "AUC:           0.7809\n"
          ]
        }
      ]
    }
  ]
}
